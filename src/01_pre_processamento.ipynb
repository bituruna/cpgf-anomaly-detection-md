{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 01 — Pré-processamento dos Dados do CPGF\n",
    "\n",
    "**Projeto:** Detecção de Anomalias no Uso do Cartão de Pagamento do Governo Federal  \n",
    "**Disciplina:** FACOM39803 — Mineração de Dados Aplicada a Finanças  \n",
    "**Etapa:** 1 de 3 — Pré-processamento\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Configuração do Ambiente\n",
    "\n",
    "Montagem do Google Drive (para execução no Colab) e importação das bibliotecas necessárias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Montagem do Google Drive (descomente no Colab) ---\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_theme(style='whitegrid', palette='muted')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print('Bibliotecas carregadas com sucesso.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carregamento dos Dados\n",
    "\n",
    "Leitura dos arquivos CSV do CPGF (a partir de 2022) armazenados na pasta `dados/`.\n",
    "\n",
    "> **Nota:** Os CSVs do Portal da Transparência utilizam separador `;` e encoding `latin-1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Definição de caminhos ---\n",
    "# Para execução local:\n",
    "DATA_DIR = os.path.join('..', 'dados')\n",
    "# Para execução no Colab (descomente e ajuste):\n",
    "# DATA_DIR = '/content/drive/MyDrive/cpgf-anomaly-detection/dados'\n",
    "\n",
    "# --- Carregamento de todos os CSVs ---\n",
    "csv_files = sorted(glob.glob(os.path.join(DATA_DIR, '*.csv')))\n",
    "print(f'Arquivos encontrados: {len(csv_files)}')\n",
    "for f in csv_files:\n",
    "    print(f'  - {os.path.basename(f)}')\n",
    "\n",
    "# --- Concatenação em um único DataFrame ---\n",
    "dfs = []\n",
    "for f in csv_files:\n",
    "    df_temp = pd.read_csv(f, sep=';', encoding='latin-1')\n",
    "    dfs.append(df_temp)\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "print(f'\\nTotal de registros carregados: {df.shape[0]:,}')\n",
    "print(f'Total de colunas: {df.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Exploração dos Dados (Aula 4)\n",
    "\n",
    "### 2.1 Fundamentação Teórica\n",
    "\n",
    "A **exploração dos dados** é o primeiro passo fundamental em qualquer projeto de mineração.\n",
    "Ela tem como objetivo compreender a natureza do conjunto de dados por meio de:\n",
    "\n",
    "- **Estatísticas resumidas** (medidas de tendência central, dispersão, quartis);\n",
    "- **Visualizações gráficas** (histogramas, boxplots, gráficos de barras) para identificar\n",
    "  distribuições, padrões e possíveis anomalias visuais nos atributos.\n",
    "\n",
    "Essa etapa permite ao analista formular hipóteses iniciais sobre os dados antes de\n",
    "aplicar qualquer técnica de mineração (Ref.: Aula 4 — Exploração de Dados)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2.2 Visão geral do DataFrame ---\n",
    "print('=== Primeiras linhas ===')\n",
    "display(df.head())\n",
    "\n",
    "print('\\n=== Informações gerais ===')\n",
    "df.info()\n",
    "\n",
    "print('\\n=== Tipos de dados ===')\n",
    "display(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2.3 Estatísticas descritivas ---\n",
    "print('=== Estatísticas descritivas (atributos numéricos) ===')\n",
    "display(df.describe())\n",
    "\n",
    "print('\\n=== Estatísticas descritivas (atributos categóricos) ===')\n",
    "display(df.describe(include='object'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2.4 Visualizações exploratórias ---\n",
    "\n",
    "# TODO: Ajustar os nomes das colunas conforme a base real do CPGF\n",
    "# Exemplo de colunas esperadas: 'VALOR_TRANSACAO', 'NOME_ORGAO', 'DATA_TRANSACAO'\n",
    "\n",
    "# Histograma do valor das transações\n",
    "# col_valor = 'VALOR_TRANSACAO'  # Ajustar conforme o CSV real\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "# df[col_valor].hist(bins=50, ax=axes[0], edgecolor='black')\n",
    "# axes[0].set_title('Distribuição dos Valores de Transação')\n",
    "# axes[0].set_xlabel('Valor (R$)')\n",
    "# axes[0].set_ylabel('Frequência')\n",
    "\n",
    "# Boxplot do valor das transações\n",
    "# sns.boxplot(x=df[col_valor], ax=axes[1])\n",
    "# axes[1].set_title('Boxplot — Valores de Transação')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Qualidade dos Dados (Aula 2)\n",
    "\n",
    "### 3.1 Fundamentação Teórica\n",
    "\n",
    "A **qualidade dos dados** impacta diretamente os resultados da mineração. Os principais\n",
    "problemas tratados nesta etapa são:\n",
    "\n",
    "#### 3.1.1 Tratamento de Ausência de Valores\n",
    "\n",
    "Valores ausentes (nulos/NaN) podem surgir por falhas de coleta ou campos opcionais.\n",
    "As estratégias padrão incluem:\n",
    "\n",
    "- **Eliminar objetos** (linhas) com valores ausentes em atributos críticos;\n",
    "- **Estimar valores** (imputação) utilizando média, mediana ou moda,\n",
    "  quando a exclusão geraria perda excessiva de dados.\n",
    "\n",
    "(Ref.: Aula 2 — Qualidade dos Dados)\n",
    "\n",
    "#### 3.1.2 Tratamento de Ruído e Dados Inconsistentes\n",
    "\n",
    "**Ruído** é uma variação aleatória ou erro nos dados medidos (ex: valor negativo\n",
    "injustificado em uma transação de compra). O tratamento de ruído envolve a limpeza\n",
    "de atributos com valores visivelmente errados.\n",
    "\n",
    "> ⚠️ **Distinção crítica:** É fundamental diferenciar **ruído** (erro de coleta/registro\n",
    "> que deve ser corrigido) de **outlier** (valor extremo genuíno que pode representar\n",
    "> uma anomalia/fraude financeira — nosso alvo de detecção). O ruído é eliminado\n",
    "> nesta etapa; os outliers são preservados para análise na Etapa 2.\n",
    "\n",
    "(Ref.: Aula 2 — Ruído e Dados Inconsistentes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.2 Análise de valores ausentes ---\n",
    "print('=== Valores ausentes por coluna ===')\n",
    "nulos = df.isnull().sum()\n",
    "nulos_pct = (nulos / len(df)) * 100\n",
    "resumo_nulos = pd.DataFrame({'Nulos': nulos, '% do Total': nulos_pct.round(2)})\n",
    "display(resumo_nulos[resumo_nulos['Nulos'] > 0].sort_values('Nulos', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.3 Tratamento de valores ausentes ---\n",
    "\n",
    "# Estratégia 1: Eliminar colunas com mais de X% de nulos\n",
    "LIMIAR_NULOS = 70  # percentual\n",
    "colunas_excluir = resumo_nulos[resumo_nulos['% do Total'] > LIMIAR_NULOS].index.tolist()\n",
    "print(f'Colunas removidas (> {LIMIAR_NULOS}% nulos): {colunas_excluir}')\n",
    "df.drop(columns=colunas_excluir, inplace=True, errors='ignore')\n",
    "\n",
    "# Estratégia 2: Para colunas numéricas restantes, imputar com mediana\n",
    "cols_numericas = df.select_dtypes(include=[np.number]).columns\n",
    "for col in cols_numericas:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        mediana = df[col].median()\n",
    "        df[col].fillna(mediana, inplace=True)\n",
    "        print(f'  Coluna \"{col}\": nulos imputados com mediana = {mediana:.2f}')\n",
    "\n",
    "# Estratégia 3: Para colunas categóricas, imputar com moda ou 'DESCONHECIDO'\n",
    "cols_categoricas = df.select_dtypes(include='object').columns\n",
    "for col in cols_categoricas:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        df[col].fillna('DESCONHECIDO', inplace=True)\n",
    "        print(f'  Coluna \"{col}\": nulos preenchidos com \"DESCONHECIDO\"')\n",
    "\n",
    "print(f'\\nValores ausentes restantes: {df.isnull().sum().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.4 Remoção de ruído e dados inconsistentes ---\n",
    "\n",
    "# TODO: Ajustar conforme as colunas reais do CSV\n",
    "# Exemplo: remover transações com valor = 0 (sem significado)\n",
    "# col_valor = 'VALOR_TRANSACAO'\n",
    "# n_antes = len(df)\n",
    "# df = df[df[col_valor] != 0]\n",
    "# print(f'Registros com valor 0 removidos: {n_antes - len(df)}')\n",
    "\n",
    "# Nota: Valores negativos podem representar estornos legítimos ou erros.\n",
    "# Avaliar e documentar a decisão:\n",
    "# n_negativos = (df[col_valor] < 0).sum()\n",
    "# print(f'Transações com valor negativo: {n_negativos}')\n",
    "# Decisão: manter valores negativos para análise ou remover como ruído?\n",
    "\n",
    "print('Etapa de remoção de ruído concluída.')\n",
    "print(f'Shape atual do DataFrame: {df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Transformação dos Dados (Aula 2)\n",
    "\n",
    "### 4.1 Fundamentação Teórica — Normalização\n",
    "\n",
    "A **normalização** é uma técnica de transformação que consiste em \"fazer o conjunto\n",
    "inteiro de valores de um atributo ter uma propriedade particular\" (Ref.: Aula 2 —\n",
    "Transformação de Dados).\n",
    "\n",
    "No contexto deste projeto, a normalização dos **atributos financeiros contínuos**\n",
    "(como valor da transação) é um **passo essencial** antes da etapa de mineração,\n",
    "pois os algoritmos de agrupamento (DBSCAN e K-Médias) baseiam-se no **cálculo de\n",
    "distância** entre pontos. Sem normalização, atributos com escalas maiores dominariam\n",
    "a medida de distância, distorcendo os resultados.\n",
    "\n",
    "Técnicas comuns de normalização:\n",
    "\n",
    "- **Min-Max Scaling:** Transforma os valores para o intervalo [0, 1].\n",
    "- **Z-Score (Standardization):** Centraliza na média 0 e desvio padrão 1.\n",
    "\n",
    "Utilizaremos **StandardScaler** (Z-Score) neste projeto, pois é mais robusto\n",
    "na presença de outliers que a normalização Min-Max."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# --- 4.2 Seleção de atributos numéricos para normalização ---\n",
    "cols_para_normalizar = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f'Colunas numéricas selecionadas para normalização: {cols_para_normalizar}')\n",
    "\n",
    "# --- 4.3 Aplicação do StandardScaler (Z-Score) ---\n",
    "scaler = StandardScaler()\n",
    "df_normalizado = df.copy()\n",
    "df_normalizado[cols_para_normalizar] = scaler.fit_transform(df[cols_para_normalizar])\n",
    "\n",
    "print('\\n=== Estatísticas após normalização ===')\n",
    "display(df_normalizado[cols_para_normalizar].describe().round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Exportação dos Dados Pré-processados\n",
    "\n",
    "O DataFrame limpo e normalizado é salvo para ser consumido pelo Notebook 02 (Mineração)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5.1 Salvar dados processados ---\n",
    "OUTPUT_DIR = os.path.join('..', 'dados')\n",
    "# Para Colab:\n",
    "# OUTPUT_DIR = '/content/drive/MyDrive/cpgf-anomaly-detection/dados'\n",
    "\n",
    "# DataFrame original limpo (sem normalização) — para análise qualitativa no Notebook 03\n",
    "df.to_csv(os.path.join(OUTPUT_DIR, 'cpgf_limpo.csv'), index=False, sep=';', encoding='utf-8')\n",
    "print('Arquivo salvo: cpgf_limpo.csv')\n",
    "\n",
    "# DataFrame normalizado — para entrada nos algoritmos de agrupamento\n",
    "df_normalizado.to_csv(os.path.join(OUTPUT_DIR, 'cpgf_normalizado.csv'), index=False, sep=';', encoding='utf-8')\n",
    "print('Arquivo salvo: cpgf_normalizado.csv')\n",
    "\n",
    "print(f'\\nShape final: {df_normalizado.shape}')\n",
    "print('\\n✅ Pré-processamento concluído com sucesso!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
