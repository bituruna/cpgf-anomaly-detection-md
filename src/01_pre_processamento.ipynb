{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pré-processamento dos Dados do CPGF\n",
        "\n",
        "**Projeto:** Detecção de Anomalias no Uso do Cartão de Pagamento do Governo Federal  \n",
        "**Disciplina:** Mineração de Dados  \n",
        "**Etapa:** 1.Pré-processamento\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Configuração do Ambiente\n",
        "\n",
        "Montagem do Google Drive para execução no Colab e importação das bibliotecas necessárias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.metrics import silhouette_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Carregamento dos Dados\n",
        "\n",
        "Leitura dos arquivos CSV do CPGF (a partir de 2022) com link disponível na pasta `dados/`.\n",
        "\n",
        "> O link é um redirecionamento para o Drive com 36 datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "caminho = '/content/drive/MyDrive/Projeto_MD_CPGF_inicial/datasets/cpgf_datasets'\n",
        "\n",
        "arquivos = [f for f in os.listdir(caminho) if f.endswith('.csv')]\n",
        "\n",
        "lista_dfs = []\n",
        "\n",
        "for arquivo in arquivos:\n",
        "    caminho_completo = os.path.join(caminho, arquivo)\n",
        "\n",
        "    df_temp = pd.read_csv(\n",
        "        caminho_completo,\n",
        "        sep=';',\n",
        "        decimal=',',\n",
        "        encoding='latin1'\n",
        "    )\n",
        "\n",
        "    lista_dfs.append(df_temp)\n",
        "\n",
        "# juntar tudo\n",
        "df = pd.concat(lista_dfs, ignore_index=True)\n",
        "\n",
        "print(\"Total de linhas:\", len(df))\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 2. Exploração dos Dados\n",
        "### 2.1 Fundamentação Teórica\n",
        "\n",
        "Utilizamos as seguintes técnicas e ferramentas:\n",
        "- altermanos a coluna VALOR TRANSAÇÃO para float.\n",
        "\n",
        "- Estatísticas resumidas (média, mediana, desvio padrão, quartis);\n",
        "- Visualizações gráficas  (boxplot nessa etapa)  para identificar\n",
        "  distribuições, padrões e possíveis anomalias visuais nos atributos.\n",
        "\n",
        "Essa parte foi necessaria antes de aplicar qualquer técnica de mineração "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 2.2 Visão geral do DataFrame ---\n",
        "print(' Primeiras linhas ')\n",
        "display(df.head())\n",
        "\n",
        "print('\\n Informações gerais')\n",
        "df.info()\n",
        "\n",
        "print('\\n Tipos de dados ')\n",
        "display(df.dtypes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['VALOR TRANSAÇÃO'] = pd.to_numeric(df['VALOR TRANSAÇÃO'], errors='coerce')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 2.3 Estatísticas descritivas ---\n",
        "print(' Estatísticas descritivas (atributos numéricos) ')\n",
        "display(df.describe())\n",
        "\n",
        "print('\\n Estatísticas descritivas (atributos categóricos) ')\n",
        "display(df.describe(include='object'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 2.4 Visualizações exploratórias ---\n",
        "\n",
        "import plotly.express as px\n",
        "fig = px.violin(df, x=\"ANO EXTRATO\", y=\"VALOR TRANSAÇÃO\", box=True)\n",
        "\n",
        "fig.update_layout(\n",
        "    width=1000,\n",
        "    height=600,\n",
        "    template=\"plotly_white\",\n",
        "    xaxis_title=\"ANO EXTRATO\",\n",
        "    yaxis_title=\"VALOR TRANSAÇÃO (R$)\",\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 3. Qualidade dos Dados\n",
        "\n",
        "Os principais problemas tratados nesta etapa são:\n",
        "\n",
        "#### 3.1. Tratamento de Ausência de Valores\n",
        "\n",
        "Valores ausentes podem surgir por falhas de coleta ou campos opcionais.\n",
        "A estratégia padrão inclui eliminar as linhas com valores ausentes em atributos críticos.\n",
        "a principio, nao eliminamos pois nao usaremos as colunas que contem dados nulos no modelo.\n",
        "\n",
        "(*discutir depois aula 2 qualidade dos dados)\n",
        "\n",
        "#### 3.2 Tratamento de Ruído e Dados Inconsistentes\n",
        "\n",
        "Para objetos, ruído é um objeto estranho.\n",
        "Para atributos, ruído refere-se à modificação dos valores originais.\n",
        "entendemos que ruído pode confundir o algoritmo.\n",
        "\n",
        "(verificar se saque pode ser um ruido)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 3.2 Análise de valores ausentes ---\n",
        "print('Valores ausentes por coluna')\n",
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 3.3 Tratamento de valores ausentes ---\n",
        "\n",
        "# Estratégia 1: Eliminar colunas com mais de X% de nulos\n",
        "LIMIAR_NULOS = 70  # percentual\n",
        "colunas_excluir = resumo_nulos[resumo_nulos['% do Total'] > LIMIAR_NULOS].index.tolist()\n",
        "print(f'Colunas removidas (> {LIMIAR_NULOS}% nulos): {colunas_excluir}')\n",
        "df.drop(columns=colunas_excluir, inplace=True, errors='ignore')\n",
        "\n",
        "# Estratégia 2: Para colunas numéricas restantes, imputar com mediana\n",
        "cols_numericas = df.select_dtypes(include=[np.number]).columns\n",
        "for col in cols_numericas:\n",
        "    if df[col].isnull().sum() > 0:\n",
        "        mediana = df[col].median()\n",
        "        df[col].fillna(mediana, inplace=True)\n",
        "        print(f'  Coluna \"{col}\": nulos imputados com mediana = {mediana:.2f}')\n",
        "\n",
        "# Estratégia 3: Para colunas categóricas, imputar com moda ou 'DESCONHECIDO'\n",
        "cols_categoricas = df.select_dtypes(include='object').columns\n",
        "for col in cols_categoricas:\n",
        "    if df[col].isnull().sum() > 0:\n",
        "        df[col].fillna('DESCONHECIDO', inplace=True)\n",
        "        print(f'  Coluna \"{col}\": nulos preenchidos com \"DESCONHECIDO\"')\n",
        "\n",
        "print(f'\\nValores ausentes restantes: {df.isnull().sum().sum()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 3.4 Remoção de ruído e dados inconsistentes ---\n",
        "\n",
        "# TODO: Ajustar conforme as colunas reais do CSV\n",
        "# Exemplo: remover transações com valor = 0 (sem significado)\n",
        "# col_valor = 'VALOR_TRANSACAO'\n",
        "# n_antes = len(df)\n",
        "# df = df[df[col_valor] != 0]\n",
        "# print(f'Registros com valor 0 removidos: {n_antes - len(df)}')\n",
        "\n",
        "# Nota: Valores negativos podem representar estornos legítimos ou erros.\n",
        "# Avaliar e documentar a decisão:\n",
        "# n_negativos = (df[col_valor] < 0).sum()\n",
        "# print(f'Transações com valor negativo: {n_negativos}')\n",
        "# Decisão: manter valores negativos para análise ou remover como ruído?\n",
        "\n",
        "print('Etapa de remoção de ruído concluída.')\n",
        "print(f'Shape atual do DataFrame: {df.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3.5 Verificação de Duplicatas\n",
        "\n",
        "Verificação de registros duplicados na base de dados conforme solicitado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 3.5 Verificação de duplicatas ---\n",
        "df[df.duplicated()]\n",
        "print(\"Total de linhas:\", len(df))\n",
        "print(\"Duplicadas:\", df.duplicated().sum())\n",
        "df[df.duplicated(keep=False)].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 4. Agregação dos Dados\n",
        "\n",
        "### 4.1 Fundamentação\n",
        "\n",
        "Nesta etapa, criamos um novo dataframe agrupando os dados por CPF do portador do cartão. O objetivo é transformar os dados  em um\n",
        "perfil de comportamento por portador do cartão, bisando que os algoritmos de mineração identifiquem padrões e anomalias do usuário.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 4.2 Agregação por CPF do portador ---\n",
        "\n",
        "df_servidor = (\n",
        "    df.groupby('CPF PORTADOR')\n",
        "      .agg(\n",
        "          total_gasto=('VALOR TRANSAÇÃO', 'sum'),\n",
        "          media_gasto=('VALOR TRANSAÇÃO', 'mean'),\n",
        "          qtd_transacoes=('VALOR TRANSAÇÃO', 'count'),\n",
        "          max_gasto=('VALOR TRANSAÇÃO', 'max')\n",
        "      )\n",
        "      .reset_index()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_servidor.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 5. Transformação dos Dados\n",
        "### 5.1 Normalização\n",
        "\n",
        "Técnicas comuns de normalização:\n",
        "\n",
        "- **Min-Max Scaling:** Transforma os valores para o intervalo [0, 1].\n",
        "- **Z-Score (Standardization):** Centraliza na média 0 e desvio padrão 1.\n",
        "\n",
        "Utilizaremos StandardScaler (Z-Score) neste projeto, pois é mais robusto\n",
        "na presença de outliers que a normalização Min-Max."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "features = ['total_gasto', 'media_gasto', 'qtd_transacoes', 'max_gasto']\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(df_servidor[features])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_scaled = pd.DataFrame(\n",
        "    X_scaled,\n",
        "    columns=features,\n",
        "    index=df_servidor.index\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_scaled.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 6. Exportação dos Dados Pré-processados\n",
        "\n",
        "O DataFrame limpo e normalizado é salvo para ser consumido pelo Notebook 02 (Mineração)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 6.1 Salvar dados processados ---\n",
        "OUTPUT_DIR = os.path.join('..', 'dados')\n",
        "# Para Colab:\n",
        "# OUTPUT_DIR = '/content/drive/MyDrive/cpgf-anomaly-detection/dados'\n",
        "\n",
        "# DataFrame original limpo (sem normalização) — para análise qualitativa no Notebook 03\n",
        "df.to_csv(os.path.join(OUTPUT_DIR, 'cpgf_limpo.csv'), index=False, sep=';', encoding='utf-8')\n",
        "print('Arquivo salvo: cpgf_limpo.csv')\n",
        "\n",
        "# DataFrame normalizado — para entrada nos algoritmos de agrupamento\n",
        "df_normalizado.to_csv(os.path.join(OUTPUT_DIR, 'cpgf_normalizado.csv'), index=False, sep=';', encoding='utf-8')\n",
        "print('Arquivo salvo: cpgf_normalizado.csv')\n",
        "\n",
        "print(f'\\nShape final: {df_normalizado.shape}')\n",
        "print('\\n✅ Pré-processamento concluído com sucesso!')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4

}
